{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicShift(nn.Module):\n",
    "    def __init__(self, displacement):\n",
    "        super().__init__()\n",
    "        self.displacement = displacement\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"torch_roll : \" , torch.roll(x, shifts=(self.displacement, self.displacement), dims=(1, 2)).shape)\n",
    "        return torch.roll(x, shifts=(self.displacement, self.displacement), dims=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8],\n",
       "        [ 9, 10, 11, 12],\n",
       "        [13, 14, 15, 16]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## torch.roll : 주어진 차원을 따라 텐서를 굴림\n",
    "x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]).view(4,4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13, 14, 15, 16],\n",
       "        [ 1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8],\n",
       "        [ 9, 10, 11, 12]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.roll(x, 1, 0) # 0행을 1행으로 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9, 10, 11, 12],\n",
       "        [13, 14, 15, 16],\n",
       "        [ 1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.roll(x, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  4,  1,  2],\n",
       "        [ 7,  8,  5,  6],\n",
       "        [11, 12,  9, 10],\n",
       "        [15, 16, 13, 14]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.roll(x, 2, 1) # 1열까지를 2열 옮기기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12,  9, 10, 11],\n",
       "        [16, 13, 14, 15],\n",
       "        [ 4,  1,  2,  3],\n",
       "        [ 8,  5,  6,  7]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.roll(x, shifts=(2, 1), dims=(0, 1))  # 0행을 2행씩 밀고 / 1열을 1 열씩 밀자  dims = 0 : 행 / 1 : 열\n",
    "#torch.roll(x, shifts=1, dims=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(window_size, displacement, upper_lower, left_right):\n",
    "    mask = torch.zeros(window_size ** 2, window_size ** 2)\n",
    "\n",
    "    if upper_lower:\n",
    "        mask[-displacement * window_size:, :-displacement * window_size] = float('-inf')\n",
    "        mask[:-displacement * window_size, -displacement * window_size:] = float('-inf')\n",
    "\n",
    "    if left_right:\n",
    "        mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n",
    "        mask[:, -displacement:, :, :-displacement] = float('-inf')\n",
    "        mask[:, :-displacement, :, -displacement:] = float('-inf')\n",
    "        mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n",
    "        print(\"mask shape:\", mask.shape)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size =  7 \n",
    "displacement = window_size // 2\n",
    "displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.zeros(window_size ** 2, window_size ** 2)\n",
    "mask #torch.Size([49, 49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[-displacement * window_size:, :-displacement * window_size] # 21 / 49-21 = 28 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size) # torch.Size([7, 7, 7, 7]) # 해제 \n",
    "mask[:, -displacement:, :, :-displacement] #torch.Size([7, 3, 7, 4])\n",
    "mask[:, :-displacement, :, -displacement:] #torch.Size([7, 4, 7, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49, 49])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)') ## 다시 결합 \n",
    "mask.shape # torch.Size([49, 49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_distances(window_size):\n",
    "    indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n",
    "    distances = indices[None, :, :] - indices[:, None, :]\n",
    "    print(\"distances.shape:\", distances.shape)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6,  6],\n",
       "         [ 6,  7],\n",
       "         [ 6,  8],\n",
       "         ...,\n",
       "         [12, 10],\n",
       "         [12, 11],\n",
       "         [12, 12]],\n",
       "\n",
       "        [[ 6,  5],\n",
       "         [ 6,  6],\n",
       "         [ 6,  7],\n",
       "         ...,\n",
       "         [12,  9],\n",
       "         [12, 10],\n",
       "         [12, 11]],\n",
       "\n",
       "        [[ 6,  4],\n",
       "         [ 6,  5],\n",
       "         [ 6,  6],\n",
       "         ...,\n",
       "         [12,  8],\n",
       "         [12,  9],\n",
       "         [12, 10]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0,  2],\n",
       "         [ 0,  3],\n",
       "         [ 0,  4],\n",
       "         ...,\n",
       "         [ 6,  6],\n",
       "         [ 6,  7],\n",
       "         [ 6,  8]],\n",
       "\n",
       "        [[ 0,  1],\n",
       "         [ 0,  2],\n",
       "         [ 0,  3],\n",
       "         ...,\n",
       "         [ 6,  5],\n",
       "         [ 6,  6],\n",
       "         [ 6,  7]],\n",
       "\n",
       "        [[ 0,  0],\n",
       "         [ 0,  1],\n",
       "         [ 0,  2],\n",
       "         ...,\n",
       "         [ 6,  4],\n",
       "         [ 6,  5],\n",
       "         [ 6,  6]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n",
    "distances = indices[None, :, :] - indices[:, None, :]\n",
    "distances + window_size - 1 # distances + window_size - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, shifted, window_size, relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        inner_dim = head_dim * heads \n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = head_dim ** -0.5 # dim scaling \n",
    "        self.window_size = window_size\n",
    "        self.relative_pos_embedding = relative_pos_embedding\n",
    "        self.shifted = shifted\n",
    "\n",
    "        if self.shifted:\n",
    "            displacement = window_size // 2\n",
    "            self.cyclic_shift = CyclicShift(-displacement)\n",
    "            self.cyclic_back_shift = CyclicShift(displacement) # back\n",
    "            self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
    "                                                             upper_lower=True, left_right=False), requires_grad=False)\n",
    "            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
    "                                                            upper_lower=False, left_right=True), requires_grad=False)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False) # dim > inner_dim * 3 channel \n",
    "  \n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            self.relative_indices = get_relative_distances(window_size) + window_size - 1\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1))\n",
    "        else:\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2))\n",
    "\n",
    "        self.to_out = nn.Linear(inner_dim, dim) # inner_dim > dim channel\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.shifted:\n",
    "            x = self.cyclic_shift(x) # CyclicShift\n",
    "        print(\"cyclic_shift\", x.shape)\n",
    "        b, n_h, n_w, _, h = *x.shape, self.heads\n",
    "        print(\"b, n_h, n_w, _, h \", b, n_h, n_w, _, h )\n",
    "        \n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1) # 분할 \n",
    "        print(\"to qkv :\" , self.to_qkv(x).shape)\n",
    "        print(\"qkv chunk :\", len(qkv))\n",
    "        nw_h = n_h // self.window_size\n",
    "        nw_w = n_w // self.window_size\n",
    "\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, 'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d',\n",
    "                                h=h, w_h=self.window_size, w_w=self.window_size), qkv) # shape 변경 \n",
    "        print(\"q:\", q.shape)\n",
    "        print(\"k:\", q.shape)\n",
    "        print(\"v:\", q.shape)\n",
    "        dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale # query and key dots \n",
    "        print(\"dots : \", dots.shape)\n",
    "        if self.relative_pos_embedding:\n",
    "            dots += self.pos_embedding[self.relative_indices[:, :, 0].type(torch.long), self.relative_indices[:, :, 1].type(torch.long)]\n",
    "        else:\n",
    "            dots += self.pos_embedding\n",
    "        print(\"relative_pos_embedding dots : \", dots.shape)\n",
    "        if self.shifted:\n",
    "            dots[:, :, -nw_w:] += self.upper_lower_mask\n",
    "            dots[:, :, nw_w - 1::nw_w] += self.left_right_mask\n",
    "        print(\"shifted dots : \", dots.shape)\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        print(\"attn : \", len(attn))\n",
    "        out = einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n",
    "        out = rearrange(out, 'b h (nw_h nw_w) (w_h w_w) d -> b (nw_h w_h) (nw_w w_w) (h d)',\n",
    "                        h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h, nw_w=nw_w)\n",
    "        out = self.to_out(out)\n",
    "\n",
    "        if self.shifted:\n",
    "            out = self.cyclic_back_shift(out) \n",
    "        print(\"out attention fin : \" ,out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 4])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "As = torch.randn(3,2,5)\n",
    "Bs = torch.randn(3,5,4)\n",
    "torch.einsum('bij,bjk->bik', As, Bs).shape # 3 2 5 * 3 5 4 >  3 2 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, mlp_dim, shifted, window_size, relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        self.attention_block = Residual(PreNorm(dim, WindowAttention(dim=dim,\n",
    "                                                                     heads=heads,\n",
    "                                                                     head_dim=head_dim,\n",
    "                                                                     shifted=shifted,\n",
    "                                                                     window_size=window_size,\n",
    "                                                                     relative_pos_embedding=relative_pos_embedding)))\n",
    "        self.mlp_block = Residual(PreNorm(dim, FeedForward(dim=dim, hidden_dim=mlp_dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attention_block(x)\n",
    "        print(\"attention_block shape:\", x.shape)\n",
    "        x = self.mlp_block(x)\n",
    "        print(\"mlp_block shape:\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
    "        super().__init__()\n",
    "        self.downscaling_factor = downscaling_factor\n",
    "        self.patch_merge = nn.Unfold(kernel_size=downscaling_factor, stride=downscaling_factor, padding=0)\n",
    "        self.linear = nn.Linear(in_channels * downscaling_factor ** 2, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        new_h, new_w = h // self.downscaling_factor, w // self.downscaling_factor\n",
    "        x = self.patch_merge(x).view(b, -1, new_h, new_w).permute(0, 2, 3, 1)\n",
    "        print(\"patch partition shape :\", x.shape)\n",
    "        x = self.linear(x)\n",
    "        print(\"patch linear shape :\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StageModule(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dimension, layers, downscaling_factor, num_heads, head_dim, window_size,\n",
    "                 relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        assert layers % 2 == 0, 'Stage layers need to be divisible by 2 for regular and shifted block.'\n",
    "\n",
    "        self.patch_partition = PatchMerging(in_channels=in_channels, out_channels=hidden_dimension,\n",
    "                                            downscaling_factor=downscaling_factor)\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(layers // 2):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
    "                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
    "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
    "                          shifted=True, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_partition(x)\n",
    "        for regular_block, shifted_block in self.layers:\n",
    "            x = regular_block(x)\n",
    "            x = shifted_block(x)\n",
    "        print(\"regular / shifted :\", x.permute(0, 3, 1, 2).shape)\n",
    "        return x.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, *, hidden_dim, layers, heads, channels=3, num_classes=1000, head_dim=32, window_size=7,\n",
    "                 downscaling_factors=(4, 2, 2, 2), relative_pos_embedding=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stage1 = StageModule(in_channels=channels, hidden_dimension=hidden_dim, layers=layers[0],\n",
    "                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage2 = StageModule(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, layers=layers[1],\n",
    "                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage3 = StageModule(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, layers=layers[2],\n",
    "                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage4 = StageModule(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, layers=layers[3],\n",
    "                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim * 8),\n",
    "            nn.Linear(hidden_dim * 8, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.stage1(img)\n",
    "        print(\"stage1 shape:\", x.shape)\n",
    "        x = self.stage2(x)\n",
    "        print(\"stage2 shape:\", x.shape)\n",
    "        x = self.stage3(x)\n",
    "        print(\"stage3 shape:\", x.shape)\n",
    "        x = self.stage4(x)\n",
    "        print(\"stage4 shape:\", x.shape)\n",
    "        x = x.mean(dim=[2, 3])\n",
    "        print(\"mean shape:\", x.shape)\n",
    "        print(\"mlp_head shape:\", self.mlp_head(x).shape)\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def swin_t(hidden_dim=96, layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
    "\n",
    "\n",
    "def swin_s(hidden_dim=96, layers=(2, 2, 18, 2), heads=(3, 6, 12, 24), **kwargs):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
    "\n",
    "\n",
    "def swin_b(hidden_dim=128, layers=(2, 2, 18, 2), heads=(4, 8, 16, 32), **kwargs):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
    "\n",
    "\n",
    "def swin_l(hidden_dim=192, layers=(2, 2, 18, 2), heads=(6, 12, 24, 48), **kwargs):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin-T model\n",
    "10장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n"
     ]
    }
   ],
   "source": [
    "net = SwinTransformer(\n",
    "    hidden_dim=96,\n",
    "    layers=(2, 2, 6, 2), #architecture swin block \n",
    "    heads=(3, 6, 12, 24),\n",
    "    channels=3,\n",
    "    num_classes=3,\n",
    "    head_dim=32,\n",
    "    window_size=7,\n",
    "    downscaling_factors=(4, 2, 2, 2),\n",
    "    relative_pos_embedding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n"
     ]
    }
   ],
   "source": [
    "net = swin_t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 224, 224])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_x = torch.randn(10, 3, 224, 224)\n",
    "dummy_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch partition shape : torch.Size([10, 56, 56, 48])\n",
      "patch linear shape : torch.Size([10, 56, 56, 96])\n",
      "cyclic_shift torch.Size([10, 56, 56, 96])\n",
      "b, n_h, n_w, _, h  10 56 56 96 3\n",
      "to qkv : torch.Size([10, 56, 56, 288])\n",
      "qkv chunk : 3\n",
      "q: torch.Size([10, 3, 64, 49, 32])\n",
      "k: torch.Size([10, 3, 64, 49, 32])\n",
      "v: torch.Size([10, 3, 64, 49, 32])\n",
      "dots :  torch.Size([10, 3, 64, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([10, 3, 64, 49, 49])\n",
      "shifted dots :  torch.Size([10, 3, 64, 49, 49])\n",
      "attn :  10\n",
      "out attention fin :  torch.Size([10, 56, 56, 96])\n",
      "attention_block shape: torch.Size([10, 56, 56, 96])\n",
      "mlp_block shape: torch.Size([10, 56, 56, 96])\n",
      "torch_roll :  torch.Size([10, 56, 56, 96])\n",
      "cyclic_shift torch.Size([10, 56, 56, 96])\n",
      "b, n_h, n_w, _, h  10 56 56 96 3\n",
      "to qkv : torch.Size([10, 56, 56, 288])\n",
      "qkv chunk : 3\n",
      "q: torch.Size([10, 3, 64, 49, 32])\n",
      "k: torch.Size([10, 3, 64, 49, 32])\n",
      "v: torch.Size([10, 3, 64, 49, 32])\n",
      "dots :  torch.Size([10, 3, 64, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([10, 3, 64, 49, 49])\n",
      "shifted dots :  torch.Size([10, 3, 64, 49, 49])\n",
      "attn :  10\n",
      "torch_roll :  torch.Size([10, 56, 56, 96])\n",
      "out attention fin :  torch.Size([10, 56, 56, 96])\n",
      "attention_block shape: torch.Size([10, 56, 56, 96])\n",
      "mlp_block shape: torch.Size([10, 56, 56, 96])\n",
      "regular / shifted : torch.Size([10, 96, 56, 56])\n",
      "stage1 shape: torch.Size([10, 96, 56, 56])\n",
      "patch partition shape : torch.Size([10, 28, 28, 384])\n",
      "patch linear shape : torch.Size([10, 28, 28, 192])\n",
      "cyclic_shift torch.Size([10, 28, 28, 192])\n",
      "b, n_h, n_w, _, h  10 28 28 192 6\n",
      "to qkv : torch.Size([10, 28, 28, 576])\n",
      "qkv chunk : 3\n",
      "q: torch.Size([10, 6, 16, 49, 32])\n",
      "k: torch.Size([10, 6, 16, 49, 32])\n",
      "v: torch.Size([10, 6, 16, 49, 32])\n",
      "dots :  torch.Size([10, 6, 16, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([10, 6, 16, 49, 49])\n",
      "shifted dots :  torch.Size([10, 6, 16, 49, 49])\n",
      "attn :  10\n",
      "out attention fin :  torch.Size([10, 28, 28, 192])\n",
      "attention_block shape: torch.Size([10, 28, 28, 192])\n",
      "mlp_block shape: torch.Size([10, 28, 28, 192])\n",
      "torch_roll :  torch.Size([10, 28, 28, 192])\n",
      "cyclic_shift torch.Size([10, 28, 28, 192])\n",
      "b, n_h, n_w, _, h  10 28 28 192 6\n",
      "to qkv : torch.Size([10, 28, 28, 576])\n",
      "qkv chunk : 3\n",
      "q: torch.Size([10, 6, 16, 49, 32])\n",
      "k: torch.Size([10, 6, 16, 49, 32])\n",
      "v: torch.Size([10, 6, 16, 49, 32])\n",
      "dots :  torch.Size([10, 6, 16, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([10, 6, 16, 49, 49])\n",
      "shifted dots :  torch.Size([10, 6, 16, 49, 49])\n",
      "attn :  10\n",
      "torch_roll :  torch.Size([10, 28, 28, 192])\n",
      "out attention fin :  torch.Size([10, 28, 28, 192])\n",
      "attention_block shape: torch.Size([10, 28, 28, 192])\n",
      "mlp_block shape: torch.Size([10, 28, 28, 192])\n",
      "regular / shifted : torch.Size([10, 192, 28, 28])\n",
      "stage2 shape: torch.Size([10, 192, 28, 28])\n",
      "patch partition shape : torch.Size([10, 14, 14, 768])\n",
      "patch linear shape : torch.Size([10, 14, 14, 384])\n",
      "cyclic_shift torch.Size([10, 14, 14, 384])\n",
      "b, n_h, n_w, _, h  10 14 14 384 12\n",
      "to qkv : torch.Size([10, 14, 14, 1152])\n",
      "qkv chunk : 3\n",
      "q: torch.Size([10, 12, 4, 49, 32])\n",
      "k: torch.Size([10, 12, 4, 49, 32])\n",
      "v: torch.Size([10, 12, 4, 49, 32])\n",
      "dots :  torch.Size([10, 12, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([10, 12, 4, 49, 49])\n",
      "shifted dots :  torch.Size([10, 12, 4, 49, 49])\n",
      "attn :  10\n",
      "out attention fin :  torch.Size([10, 14, 14, 384])\n",
      "attention_block shape: torch.Size([10, 14, 14, 384])\n",
      "mlp_block shape: torch.Size([10, 14, 14, 384])\n",
      "torch_roll :  torch.Size([10, 14, 14, 384])\n",
      "cyclic_shift torch.Size([10, 14, 14, 384])\n",
      "b, n_h, n_w, _, h  10 14 14 384 12\n",
      "to qkv : torch.Size([10, 14, 14, 1152])\n",
      "qkv chunk : 3\n",
      "q: torch.Size([10, 12, 4, 49, 32])\n",
      "k: torch.Size([10, 12, 4, 49, 32])\n",
      "v: torch.Size([10, 12, 4, 49, 32])\n",
      "dots :  torch.Size([10, 12, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([10, 12, 4, 49, 49])\n",
      "shifted dots :  torch.Size([10, 12, 4, 49, 49])\n",
      "attn :  10\n",
      "torch_roll :  torch.Size([10, 14, 14, 384])\n",
      "out attention fin :  torch.Size([10, 14, 14, 384])\n",
      "attention_block shape: torch.Size([10, 14, 14, 384])\n",
      "mlp_block shape: torch.Size([10, 14, 14, 384])\n",
      "cyclic_shift torch.Size([10, 14, 14, 384])\n",
      "b, n_h, n_w, _, h  10 14 14 384 12\n",
      "to qkv : torch.Size([10, 14, 14, 1152])\n",
      "qkv chunk : 3\n",
      "q: torch.Size([10, 12, 4, 49, 32])\n",
      "k: torch.Size([10, 12, 4, 49, 32])\n",
      "v: torch.Size([10, 12, 4, 49, 32])\n",
      "dots :  torch.Size([10, 12, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([10, 12, 4, 49, 49])\n",
      "shifted dots :  torch.Size([10, 12, 4, 49, 49])\n",
      "attn :  10\n",
      "out attention fin :  torch.Size([10, 14, 14, 384])\n",
      "attention_block shape: torch.Size([10, 14, 14, 384])\n",
      "mlp_block shape: torch.Size([10, 14, 14, 384])\n",
      "torch_roll :  torch.Size([10, 14, 14, 384])\n",
      "cyclic_shift torch.Size([10, 14, 14, 384])\n",
      "b, n_h, n_w, _, h  10 14 14 384 12\n",
      "to qkv : torch.Size([10, 14, 14, 1152])\n",
      "qkv chunk : 3\n",
      "q: torch.Size([10, 12, 4, 49, 32])\n",
      "k: torch.Size([10, 12, 4, 49, 32])\n",
      "v: torch.Size([10, 12, 4, 49, 32])\n",
      "dots :  torch.Size([10, 12, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([10, 12, 4, 49, 49])\n",
      "shifted dots :  torch.Size([10, 12, 4, 49, 49])\n",
      "attn :  10\n",
      "torch_roll :  torch.Size([10, 14, 14, 384])\n",
      "out attention fin :  torch.Size([10, 14, 14, 384])\n",
      "attention_block shape: torch.Size([10, 14, 14, 384])\n",
      "mlp_block shape: torch.Size([10, 14, 14, 384])\n",
      "cyclic_shift torch.Size([10, 14, 14, 384])\n",
      "b, n_h, n_w, _, h  10 14 14 384 12\n",
      "to qkv : torch.Size([10, 14, 14, 1152])\n",
      "qkv chunk : 3\n",
      "q: torch.Size([10, 12, 4, 49, 32])\n",
      "k: torch.Size([10, 12, 4, 49, 32])\n",
      "v: torch.Size([10, 12, 4, 49, 32])\n",
      "dots :  torch.Size([10, 12, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([10, 12, 4, 49, 49])\n",
      "shifted dots :  torch.Size([10, 12, 4, 49, 49])\n",
      "attn :  10\n",
      "out attention fin :  torch.Size([10, 14, 14, 384])\n",
      "attention_block shape: torch.Size([10, 14, 14, 384])\n",
      "mlp_block shape: torch.Size([10, 14, 14, 384])\n",
      "torch_roll :  torch.Size([10, 14, 14, 384])\n",
      "cyclic_shift torch.Size([10, 14, 14, 384])\n",
      "b, n_h, n_w, _, h  10 14 14 384 12\n",
      "to qkv : torch.Size([10, 14, 14, 1152])\n",
      "qkv chunk : 3\n",
      "q: torch.Size([10, 12, 4, 49, 32])\n",
      "k: torch.Size([10, 12, 4, 49, 32])\n",
      "v: torch.Size([10, 12, 4, 49, 32])\n",
      "dots :  torch.Size([10, 12, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([10, 12, 4, 49, 49])\n",
      "shifted dots :  torch.Size([10, 12, 4, 49, 49])\n",
      "attn :  10\n",
      "torch_roll :  torch.Size([10, 14, 14, 384])\n",
      "out attention fin :  torch.Size([10, 14, 14, 384])\n",
      "attention_block shape: torch.Size([10, 14, 14, 384])\n",
      "mlp_block shape: torch.Size([10, 14, 14, 384])\n",
      "regular / shifted : torch.Size([10, 384, 14, 14])\n",
      "stage3 shape: torch.Size([10, 384, 14, 14])\n",
      "patch partition shape : torch.Size([10, 7, 7, 1536])\n",
      "patch linear shape : torch.Size([10, 7, 7, 768])\n",
      "cyclic_shift torch.Size([10, 7, 7, 768])\n",
      "b, n_h, n_w, _, h  10 7 7 768 24\n",
      "to qkv : torch.Size([10, 7, 7, 2304])\n",
      "qkv chunk : 3\n",
      "q: torch.Size([10, 24, 1, 49, 32])\n",
      "k: torch.Size([10, 24, 1, 49, 32])\n",
      "v: torch.Size([10, 24, 1, 49, 32])\n",
      "dots :  torch.Size([10, 24, 1, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([10, 24, 1, 49, 49])\n",
      "shifted dots :  torch.Size([10, 24, 1, 49, 49])\n",
      "attn :  10\n",
      "out attention fin :  torch.Size([10, 7, 7, 768])\n",
      "attention_block shape: torch.Size([10, 7, 7, 768])\n",
      "mlp_block shape: torch.Size([10, 7, 7, 768])\n",
      "torch_roll :  torch.Size([10, 7, 7, 768])\n",
      "cyclic_shift torch.Size([10, 7, 7, 768])\n",
      "b, n_h, n_w, _, h  10 7 7 768 24\n",
      "to qkv : torch.Size([10, 7, 7, 2304])\n",
      "qkv chunk : 3\n",
      "q: torch.Size([10, 24, 1, 49, 32])\n",
      "k: torch.Size([10, 24, 1, 49, 32])\n",
      "v: torch.Size([10, 24, 1, 49, 32])\n",
      "dots :  torch.Size([10, 24, 1, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([10, 24, 1, 49, 49])\n",
      "shifted dots :  torch.Size([10, 24, 1, 49, 49])\n",
      "attn :  10\n",
      "torch_roll :  torch.Size([10, 7, 7, 768])\n",
      "out attention fin :  torch.Size([10, 7, 7, 768])\n",
      "attention_block shape: torch.Size([10, 7, 7, 768])\n",
      "mlp_block shape: torch.Size([10, 7, 7, 768])\n",
      "regular / shifted : torch.Size([10, 768, 7, 7])\n",
      "stage4 shape: torch.Size([10, 768, 7, 7])\n",
      "mean shape: torch.Size([10, 768])\n",
      "mlp_head shape: torch.Size([10, 1000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0329,  0.4933,  1.0318,  ..., -0.3727,  0.2248,  0.1930],\n",
       "        [-0.2262,  0.3669,  1.2623,  ..., -0.2189,  0.2762,  0.1446],\n",
       "        [-0.0309,  0.5681,  1.1785,  ..., -0.1589,  0.1900, -0.0177],\n",
       "        ...,\n",
       "        [-0.1591,  0.4525,  1.2227,  ..., -0.3526,  0.3906,  0.2651],\n",
       "        [-0.1657,  0.3989,  1.3800,  ..., -0.1764,  0.3076,  0.2358],\n",
       "        [-0.0176,  0.4378,  1.1322,  ..., -0.1509,  0.3652,  0.1394]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = net(dummy_x)  # (1,3)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformer(\n",
      "  (stage1): StageModule(\n",
      "    (patch_partition): PatchMerging(\n",
      "      (patch_merge): Unfold(kernel_size=4, dilation=1, padding=0, stride=4)\n",
      "      (linear): Linear(in_features=48, out_features=96, bias=True)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (to_qkv): Linear(in_features=96, out_features=288, bias=False)\n",
      "                (to_out): Linear(in_features=96, out_features=96, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=96, out_features=384, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=384, out_features=96, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (cyclic_shift): CyclicShift()\n",
      "                (cyclic_back_shift): CyclicShift()\n",
      "                (to_qkv): Linear(in_features=96, out_features=288, bias=False)\n",
      "                (to_out): Linear(in_features=96, out_features=96, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=96, out_features=384, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=384, out_features=96, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (stage2): StageModule(\n",
      "    (patch_partition): PatchMerging(\n",
      "      (patch_merge): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)\n",
      "      (linear): Linear(in_features=384, out_features=192, bias=True)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "                (to_out): Linear(in_features=192, out_features=192, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=768, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (cyclic_shift): CyclicShift()\n",
      "                (cyclic_back_shift): CyclicShift()\n",
      "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
      "                (to_out): Linear(in_features=192, out_features=192, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=192, out_features=768, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (stage3): StageModule(\n",
      "    (patch_partition): PatchMerging(\n",
      "      (patch_merge): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)\n",
      "      (linear): Linear(in_features=768, out_features=384, bias=True)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
      "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (cyclic_shift): CyclicShift()\n",
      "                (cyclic_back_shift): CyclicShift()\n",
      "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
      "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): ModuleList(\n",
      "        (0): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
      "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (cyclic_shift): CyclicShift()\n",
      "                (cyclic_back_shift): CyclicShift()\n",
      "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
      "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): ModuleList(\n",
      "        (0): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
      "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (cyclic_shift): CyclicShift()\n",
      "                (cyclic_back_shift): CyclicShift()\n",
      "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
      "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (stage4): StageModule(\n",
      "    (patch_partition): PatchMerging(\n",
      "      (patch_merge): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)\n",
      "      (linear): Linear(in_features=1536, out_features=768, bias=True)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): ModuleList(\n",
      "        (0): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (to_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "                (to_out): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): SwinBlock(\n",
      "          (attention_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): WindowAttention(\n",
      "                (cyclic_shift): CyclicShift()\n",
      "                (cyclic_back_shift): CyclicShift()\n",
      "                (to_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
      "                (to_out): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (mlp_block): Residual(\n",
      "            (fn): PreNorm(\n",
      "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fn): FeedForward(\n",
      "                (net): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (1): GELU()\n",
      "                  (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mlp_head): Sequential(\n",
      "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=768, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1000])\n"
     ]
    }
   ],
   "source": [
    "print(logits.shape) # 10장의 1000 개 class 예측 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin-B model\n",
    "5장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n"
     ]
    }
   ],
   "source": [
    "net_B = SwinTransformer(\n",
    "    hidden_dim=96,\n",
    "    layers=(2, 2, 6, 2), #architecture swin block \n",
    "    heads=(3, 6, 12, 24),\n",
    "    channels=3,\n",
    "    num_classes=3,\n",
    "    head_dim=32,\n",
    "    window_size=7,\n",
    "    downscaling_factors=(4, 2, 2, 2),\n",
    "    relative_pos_embedding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "distances.shape: torch.Size([49, 49, 2])\n",
      "mask shape: torch.Size([49, 49])\n",
      "distances.shape: torch.Size([49, 49, 2])\n"
     ]
    }
   ],
   "source": [
    "net_B = swin_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 224, 224])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_y = torch.randn(5, 3, 224, 224)\n",
    "dummy_y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch partition shape : torch.Size([5, 56, 56, 48])\n",
      "patch linear shape : torch.Size([5, 56, 56, 128])\n",
      "cyclic_shift torch.Size([5, 56, 56, 128])\n",
      "b, n_h, n_w, _, h  5 56 56 128 4\n",
      "qkv: 3\n",
      "q: torch.Size([5, 4, 64, 49, 32])\n",
      "k: torch.Size([5, 4, 64, 49, 32])\n",
      "v: torch.Size([5, 4, 64, 49, 32])\n",
      "dots :  torch.Size([5, 4, 64, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 4, 64, 49, 49])\n",
      "shifted dots :  torch.Size([5, 4, 64, 49, 49])\n",
      "attn :  5\n",
      "out attention fin :  torch.Size([5, 56, 56, 128])\n",
      "attention_block shape: torch.Size([5, 56, 56, 128])\n",
      "mlp_block shape: torch.Size([5, 56, 56, 128])\n",
      "torch_roll :  torch.Size([5, 56, 56, 128])\n",
      "cyclic_shift torch.Size([5, 56, 56, 128])\n",
      "b, n_h, n_w, _, h  5 56 56 128 4\n",
      "qkv: 3\n",
      "q: torch.Size([5, 4, 64, 49, 32])\n",
      "k: torch.Size([5, 4, 64, 49, 32])\n",
      "v: torch.Size([5, 4, 64, 49, 32])\n",
      "dots :  torch.Size([5, 4, 64, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 4, 64, 49, 49])\n",
      "shifted dots :  torch.Size([5, 4, 64, 49, 49])\n",
      "attn :  5\n",
      "torch_roll :  torch.Size([5, 56, 56, 128])\n",
      "out attention fin :  torch.Size([5, 56, 56, 128])\n",
      "attention_block shape: torch.Size([5, 56, 56, 128])\n",
      "mlp_block shape: torch.Size([5, 56, 56, 128])\n",
      "regular / shifted : torch.Size([5, 128, 56, 56])\n",
      "stage1 shape: torch.Size([5, 128, 56, 56])\n",
      "patch partition shape : torch.Size([5, 28, 28, 512])\n",
      "patch linear shape : torch.Size([5, 28, 28, 256])\n",
      "cyclic_shift torch.Size([5, 28, 28, 256])\n",
      "b, n_h, n_w, _, h  5 28 28 256 8\n",
      "qkv: 3\n",
      "q: torch.Size([5, 8, 16, 49, 32])\n",
      "k: torch.Size([5, 8, 16, 49, 32])\n",
      "v: torch.Size([5, 8, 16, 49, 32])\n",
      "dots :  torch.Size([5, 8, 16, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 8, 16, 49, 49])\n",
      "shifted dots :  torch.Size([5, 8, 16, 49, 49])\n",
      "attn :  5\n",
      "out attention fin :  torch.Size([5, 28, 28, 256])\n",
      "attention_block shape: torch.Size([5, 28, 28, 256])\n",
      "mlp_block shape: torch.Size([5, 28, 28, 256])\n",
      "torch_roll :  torch.Size([5, 28, 28, 256])\n",
      "cyclic_shift torch.Size([5, 28, 28, 256])\n",
      "b, n_h, n_w, _, h  5 28 28 256 8\n",
      "qkv: 3\n",
      "q: torch.Size([5, 8, 16, 49, 32])\n",
      "k: torch.Size([5, 8, 16, 49, 32])\n",
      "v: torch.Size([5, 8, 16, 49, 32])\n",
      "dots :  torch.Size([5, 8, 16, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 8, 16, 49, 49])\n",
      "shifted dots :  torch.Size([5, 8, 16, 49, 49])\n",
      "attn :  5\n",
      "torch_roll :  torch.Size([5, 28, 28, 256])\n",
      "out attention fin :  torch.Size([5, 28, 28, 256])\n",
      "attention_block shape: torch.Size([5, 28, 28, 256])\n",
      "mlp_block shape: torch.Size([5, 28, 28, 256])\n",
      "regular / shifted : torch.Size([5, 256, 28, 28])\n",
      "stage2 shape: torch.Size([5, 256, 28, 28])\n",
      "patch partition shape : torch.Size([5, 14, 14, 1024])\n",
      "patch linear shape : torch.Size([5, 14, 14, 512])\n",
      "cyclic_shift torch.Size([5, 14, 14, 512])\n",
      "b, n_h, n_w, _, h  5 14 14 512 16\n",
      "qkv: 3\n",
      "q: torch.Size([5, 16, 4, 49, 32])\n",
      "k: torch.Size([5, 16, 4, 49, 32])\n",
      "v: torch.Size([5, 16, 4, 49, 32])\n",
      "dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "shifted dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "attn :  5\n",
      "out attention fin :  torch.Size([5, 14, 14, 512])\n",
      "attention_block shape: torch.Size([5, 14, 14, 512])\n",
      "mlp_block shape: torch.Size([5, 14, 14, 512])\n",
      "torch_roll :  torch.Size([5, 14, 14, 512])\n",
      "cyclic_shift torch.Size([5, 14, 14, 512])\n",
      "b, n_h, n_w, _, h  5 14 14 512 16\n",
      "qkv: 3\n",
      "q: torch.Size([5, 16, 4, 49, 32])\n",
      "k: torch.Size([5, 16, 4, 49, 32])\n",
      "v: torch.Size([5, 16, 4, 49, 32])\n",
      "dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "shifted dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "attn :  5\n",
      "torch_roll :  torch.Size([5, 14, 14, 512])\n",
      "out attention fin :  torch.Size([5, 14, 14, 512])\n",
      "attention_block shape: torch.Size([5, 14, 14, 512])\n",
      "mlp_block shape: torch.Size([5, 14, 14, 512])\n",
      "cyclic_shift torch.Size([5, 14, 14, 512])\n",
      "b, n_h, n_w, _, h  5 14 14 512 16\n",
      "qkv: 3\n",
      "q: torch.Size([5, 16, 4, 49, 32])\n",
      "k: torch.Size([5, 16, 4, 49, 32])\n",
      "v: torch.Size([5, 16, 4, 49, 32])\n",
      "dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "shifted dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "attn :  5\n",
      "out attention fin :  torch.Size([5, 14, 14, 512])\n",
      "attention_block shape: torch.Size([5, 14, 14, 512])\n",
      "mlp_block shape: torch.Size([5, 14, 14, 512])\n",
      "torch_roll :  torch.Size([5, 14, 14, 512])\n",
      "cyclic_shift torch.Size([5, 14, 14, 512])\n",
      "b, n_h, n_w, _, h  5 14 14 512 16\n",
      "qkv: 3\n",
      "q: torch.Size([5, 16, 4, 49, 32])\n",
      "k: torch.Size([5, 16, 4, 49, 32])\n",
      "v: torch.Size([5, 16, 4, 49, 32])\n",
      "dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "shifted dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "attn :  5\n",
      "torch_roll :  torch.Size([5, 14, 14, 512])\n",
      "out attention fin :  torch.Size([5, 14, 14, 512])\n",
      "attention_block shape: torch.Size([5, 14, 14, 512])\n",
      "mlp_block shape: torch.Size([5, 14, 14, 512])\n",
      "cyclic_shift torch.Size([5, 14, 14, 512])\n",
      "b, n_h, n_w, _, h  5 14 14 512 16\n",
      "qkv: 3\n",
      "q: torch.Size([5, 16, 4, 49, 32])\n",
      "k: torch.Size([5, 16, 4, 49, 32])\n",
      "v: torch.Size([5, 16, 4, 49, 32])\n",
      "dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "shifted dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "attn :  5\n",
      "out attention fin :  torch.Size([5, 14, 14, 512])\n",
      "attention_block shape: torch.Size([5, 14, 14, 512])\n",
      "mlp_block shape: torch.Size([5, 14, 14, 512])\n",
      "torch_roll :  torch.Size([5, 14, 14, 512])\n",
      "cyclic_shift torch.Size([5, 14, 14, 512])\n",
      "b, n_h, n_w, _, h  5 14 14 512 16\n",
      "qkv: 3\n",
      "q: torch.Size([5, 16, 4, 49, 32])\n",
      "k: torch.Size([5, 16, 4, 49, 32])\n",
      "v: torch.Size([5, 16, 4, 49, 32])\n",
      "dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "shifted dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "attn :  5\n",
      "torch_roll :  torch.Size([5, 14, 14, 512])\n",
      "out attention fin :  torch.Size([5, 14, 14, 512])\n",
      "attention_block shape: torch.Size([5, 14, 14, 512])\n",
      "mlp_block shape: torch.Size([5, 14, 14, 512])\n",
      "cyclic_shift torch.Size([5, 14, 14, 512])\n",
      "b, n_h, n_w, _, h  5 14 14 512 16\n",
      "qkv: 3\n",
      "q: torch.Size([5, 16, 4, 49, 32])\n",
      "k: torch.Size([5, 16, 4, 49, 32])\n",
      "v: torch.Size([5, 16, 4, 49, 32])\n",
      "dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "shifted dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "attn :  5\n",
      "out attention fin :  torch.Size([5, 14, 14, 512])\n",
      "attention_block shape: torch.Size([5, 14, 14, 512])\n",
      "mlp_block shape: torch.Size([5, 14, 14, 512])\n",
      "torch_roll :  torch.Size([5, 14, 14, 512])\n",
      "cyclic_shift torch.Size([5, 14, 14, 512])\n",
      "b, n_h, n_w, _, h  5 14 14 512 16\n",
      "qkv: 3\n",
      "q: torch.Size([5, 16, 4, 49, 32])\n",
      "k: torch.Size([5, 16, 4, 49, 32])\n",
      "v: torch.Size([5, 16, 4, 49, 32])\n",
      "dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "shifted dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "attn :  5\n",
      "torch_roll :  torch.Size([5, 14, 14, 512])\n",
      "out attention fin :  torch.Size([5, 14, 14, 512])\n",
      "attention_block shape: torch.Size([5, 14, 14, 512])\n",
      "mlp_block shape: torch.Size([5, 14, 14, 512])\n",
      "cyclic_shift torch.Size([5, 14, 14, 512])\n",
      "b, n_h, n_w, _, h  5 14 14 512 16\n",
      "qkv: 3\n",
      "q: torch.Size([5, 16, 4, 49, 32])\n",
      "k: torch.Size([5, 16, 4, 49, 32])\n",
      "v: torch.Size([5, 16, 4, 49, 32])\n",
      "dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "shifted dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "attn :  5\n",
      "out attention fin :  torch.Size([5, 14, 14, 512])\n",
      "attention_block shape: torch.Size([5, 14, 14, 512])\n",
      "mlp_block shape: torch.Size([5, 14, 14, 512])\n",
      "torch_roll :  torch.Size([5, 14, 14, 512])\n",
      "cyclic_shift torch.Size([5, 14, 14, 512])\n",
      "b, n_h, n_w, _, h  5 14 14 512 16\n",
      "qkv: 3\n",
      "q: torch.Size([5, 16, 4, 49, 32])\n",
      "k: torch.Size([5, 16, 4, 49, 32])\n",
      "v: torch.Size([5, 16, 4, 49, 32])\n",
      "dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "shifted dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "attn :  5\n",
      "torch_roll :  torch.Size([5, 14, 14, 512])\n",
      "out attention fin :  torch.Size([5, 14, 14, 512])\n",
      "attention_block shape: torch.Size([5, 14, 14, 512])\n",
      "mlp_block shape: torch.Size([5, 14, 14, 512])\n",
      "cyclic_shift torch.Size([5, 14, 14, 512])\n",
      "b, n_h, n_w, _, h  5 14 14 512 16\n",
      "qkv: 3\n",
      "q: torch.Size([5, 16, 4, 49, 32])\n",
      "k: torch.Size([5, 16, 4, 49, 32])\n",
      "v: torch.Size([5, 16, 4, 49, 32])\n",
      "dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "shifted dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "attn :  5\n",
      "out attention fin :  torch.Size([5, 14, 14, 512])\n",
      "attention_block shape: torch.Size([5, 14, 14, 512])\n",
      "mlp_block shape: torch.Size([5, 14, 14, 512])\n",
      "torch_roll :  torch.Size([5, 14, 14, 512])\n",
      "cyclic_shift torch.Size([5, 14, 14, 512])\n",
      "b, n_h, n_w, _, h  5 14 14 512 16\n",
      "qkv: 3\n",
      "q: torch.Size([5, 16, 4, 49, 32])\n",
      "k: torch.Size([5, 16, 4, 49, 32])\n",
      "v: torch.Size([5, 16, 4, 49, 32])\n",
      "dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "shifted dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "attn :  5\n",
      "torch_roll :  torch.Size([5, 14, 14, 512])\n",
      "out attention fin :  torch.Size([5, 14, 14, 512])\n",
      "attention_block shape: torch.Size([5, 14, 14, 512])\n",
      "mlp_block shape: torch.Size([5, 14, 14, 512])\n",
      "cyclic_shift torch.Size([5, 14, 14, 512])\n",
      "b, n_h, n_w, _, h  5 14 14 512 16\n",
      "qkv: 3\n",
      "q: torch.Size([5, 16, 4, 49, 32])\n",
      "k: torch.Size([5, 16, 4, 49, 32])\n",
      "v: torch.Size([5, 16, 4, 49, 32])\n",
      "dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "shifted dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "attn :  5\n",
      "out attention fin :  torch.Size([5, 14, 14, 512])\n",
      "attention_block shape: torch.Size([5, 14, 14, 512])\n",
      "mlp_block shape: torch.Size([5, 14, 14, 512])\n",
      "torch_roll :  torch.Size([5, 14, 14, 512])\n",
      "cyclic_shift torch.Size([5, 14, 14, 512])\n",
      "b, n_h, n_w, _, h  5 14 14 512 16\n",
      "qkv: 3\n",
      "q: torch.Size([5, 16, 4, 49, 32])\n",
      "k: torch.Size([5, 16, 4, 49, 32])\n",
      "v: torch.Size([5, 16, 4, 49, 32])\n",
      "dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "shifted dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "attn :  5\n",
      "torch_roll :  torch.Size([5, 14, 14, 512])\n",
      "out attention fin :  torch.Size([5, 14, 14, 512])\n",
      "attention_block shape: torch.Size([5, 14, 14, 512])\n",
      "mlp_block shape: torch.Size([5, 14, 14, 512])\n",
      "cyclic_shift torch.Size([5, 14, 14, 512])\n",
      "b, n_h, n_w, _, h  5 14 14 512 16\n",
      "qkv: 3\n",
      "q: torch.Size([5, 16, 4, 49, 32])\n",
      "k: torch.Size([5, 16, 4, 49, 32])\n",
      "v: torch.Size([5, 16, 4, 49, 32])\n",
      "dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "shifted dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "attn :  5\n",
      "out attention fin :  torch.Size([5, 14, 14, 512])\n",
      "attention_block shape: torch.Size([5, 14, 14, 512])\n",
      "mlp_block shape: torch.Size([5, 14, 14, 512])\n",
      "torch_roll :  torch.Size([5, 14, 14, 512])\n",
      "cyclic_shift torch.Size([5, 14, 14, 512])\n",
      "b, n_h, n_w, _, h  5 14 14 512 16\n",
      "qkv: 3\n",
      "q: torch.Size([5, 16, 4, 49, 32])\n",
      "k: torch.Size([5, 16, 4, 49, 32])\n",
      "v: torch.Size([5, 16, 4, 49, 32])\n",
      "dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "shifted dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "attn :  5\n",
      "torch_roll :  torch.Size([5, 14, 14, 512])\n",
      "out attention fin :  torch.Size([5, 14, 14, 512])\n",
      "attention_block shape: torch.Size([5, 14, 14, 512])\n",
      "mlp_block shape: torch.Size([5, 14, 14, 512])\n",
      "cyclic_shift torch.Size([5, 14, 14, 512])\n",
      "b, n_h, n_w, _, h  5 14 14 512 16\n",
      "qkv: 3\n",
      "q: torch.Size([5, 16, 4, 49, 32])\n",
      "k: torch.Size([5, 16, 4, 49, 32])\n",
      "v: torch.Size([5, 16, 4, 49, 32])\n",
      "dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "shifted dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "attn :  5\n",
      "out attention fin :  torch.Size([5, 14, 14, 512])\n",
      "attention_block shape: torch.Size([5, 14, 14, 512])\n",
      "mlp_block shape: torch.Size([5, 14, 14, 512])\n",
      "torch_roll :  torch.Size([5, 14, 14, 512])\n",
      "cyclic_shift torch.Size([5, 14, 14, 512])\n",
      "b, n_h, n_w, _, h  5 14 14 512 16\n",
      "qkv: 3\n",
      "q: torch.Size([5, 16, 4, 49, 32])\n",
      "k: torch.Size([5, 16, 4, 49, 32])\n",
      "v: torch.Size([5, 16, 4, 49, 32])\n",
      "dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "shifted dots :  torch.Size([5, 16, 4, 49, 49])\n",
      "attn :  5\n",
      "torch_roll :  torch.Size([5, 14, 14, 512])\n",
      "out attention fin :  torch.Size([5, 14, 14, 512])\n",
      "attention_block shape: torch.Size([5, 14, 14, 512])\n",
      "mlp_block shape: torch.Size([5, 14, 14, 512])\n",
      "regular / shifted : torch.Size([5, 512, 14, 14])\n",
      "stage3 shape: torch.Size([5, 512, 14, 14])\n",
      "patch partition shape : torch.Size([5, 7, 7, 2048])\n",
      "patch linear shape : torch.Size([5, 7, 7, 1024])\n",
      "cyclic_shift torch.Size([5, 7, 7, 1024])\n",
      "b, n_h, n_w, _, h  5 7 7 1024 32\n",
      "qkv: 3\n",
      "q: torch.Size([5, 32, 1, 49, 32])\n",
      "k: torch.Size([5, 32, 1, 49, 32])\n",
      "v: torch.Size([5, 32, 1, 49, 32])\n",
      "dots :  torch.Size([5, 32, 1, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 32, 1, 49, 49])\n",
      "shifted dots :  torch.Size([5, 32, 1, 49, 49])\n",
      "attn :  5\n",
      "out attention fin :  torch.Size([5, 7, 7, 1024])\n",
      "attention_block shape: torch.Size([5, 7, 7, 1024])\n",
      "mlp_block shape: torch.Size([5, 7, 7, 1024])\n",
      "torch_roll :  torch.Size([5, 7, 7, 1024])\n",
      "cyclic_shift torch.Size([5, 7, 7, 1024])\n",
      "b, n_h, n_w, _, h  5 7 7 1024 32\n",
      "qkv: 3\n",
      "q: torch.Size([5, 32, 1, 49, 32])\n",
      "k: torch.Size([5, 32, 1, 49, 32])\n",
      "v: torch.Size([5, 32, 1, 49, 32])\n",
      "dots :  torch.Size([5, 32, 1, 49, 49])\n",
      "relative_pos_embedding dots :  torch.Size([5, 32, 1, 49, 49])\n",
      "shifted dots :  torch.Size([5, 32, 1, 49, 49])\n",
      "attn :  5\n",
      "torch_roll :  torch.Size([5, 7, 7, 1024])\n",
      "out attention fin :  torch.Size([5, 7, 7, 1024])\n",
      "attention_block shape: torch.Size([5, 7, 7, 1024])\n",
      "mlp_block shape: torch.Size([5, 7, 7, 1024])\n",
      "regular / shifted : torch.Size([5, 1024, 7, 7])\n",
      "stage4 shape: torch.Size([5, 1024, 7, 7])\n",
      "mean shape: torch.Size([5, 1024])\n",
      "mlp_head shape: torch.Size([5, 1000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0724, -0.5715, -0.1288,  ..., -1.0682, -0.7317, -0.0189],\n",
       "        [-1.1644, -0.4917, -0.2390,  ..., -0.9966, -0.8087, -0.0611],\n",
       "        [-1.0819, -0.5840, -0.0381,  ..., -0.9123, -0.6584, -0.2453],\n",
       "        [-1.1570, -0.3997, -0.1508,  ..., -0.9247, -0.6607, -0.1020],\n",
       "        [-1.1046, -0.4589, -0.2040,  ..., -0.9864, -0.7731, -0.1307]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_y = net_B(dummy_y)  \n",
    "logits_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1000])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
